{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0ad23e-c04a-428e-af47-38a54042f041",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#데이터 불러오기\n",
    "book_data = pd.read_csv(r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\교보크롤링(소설).csv\", encoding = \"cp949\")\n",
    "\n",
    "print(len(book_data))\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "stop_word_path = r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\stopword.txt\"\n",
    "with open(stop_word_path, \"r\", encoding='utf-8') as file:\n",
    "    stop_words = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba68967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#특수문자 숫자 제거\n",
    "def remove_char_stopwords(text,stop_words):\n",
    "    # 한국어 문자와 공백만 허용\n",
    "    # 정규식으로 찾은 문자열을 ' '공백 처리\n",
    "    text = re.sub(r'[^\\w\\s가-힣]', ' ', text)  # 특수 문자제거\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    filtered_words = [word for word in words if len(word)>1]\n",
    "    \n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "\n",
    "    # 원래의 공백을 복원하기 위해 공백을 원래대로 유지하는 로직 추가\n",
    "    # 단어 사이의 공백을 원래 상태로 유지\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    extract_nouns = okt.nouns(cleaned_text)\n",
    "    \n",
    "    filtered_nouns= [word for word in extract_nouns if word not in stop_words]\n",
    "\n",
    "    final_word_set = set(noun for noun in filtered_nouns if len(noun) > 1)\n",
    "    \n",
    "    LDA_data_set = list(final_word_set)\n",
    "    \n",
    "    return LDA_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae84c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda_analysis(documents, num_topics=5, num_top_words=10):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    all_words_set = set()\n",
    "    \n",
    "    for topic in lda.components_:\n",
    "        top_words_idx = topic.argsort()[-num_top_words:][::-1]\n",
    "        top_words = [vectorizer.get_feature_names_out()[index] for index in top_words_idx]\n",
    "        all_words_set.update(top_words)\n",
    "    \n",
    "    return ','.join(all_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a0e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recommended_words_column(df, stop_words):\n",
    "    recommended_words = []\n",
    "    for i in range(len(df)):\n",
    "        data = df.iloc[i, 4]\n",
    "        LDA_data = remove_char_stopwords(data, stop_words)\n",
    "        final_data = perform_lda_analysis(LDA_data, num_topics=5, num_top_words=10)\n",
    "        recommended_words.append(final_data)\n",
    "    df['추천단어'] = recommended_words\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94590865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엑셀 파일로 저장되었습니다: C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어(LDA기반).csv\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임에 새로운 열 추가\n",
    "book_data_with_words = add_recommended_words_column(book_data, stop_words)\n",
    "\n",
    "# 결과를 엑셀 파일로 저장\n",
    "output_path = r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어(LDA기반).csv\"\n",
    "book_data_with_words.to_csv(output_path, index=False, encoding='cp949')\n",
    "\n",
    "print(f\"엑셀 파일로 저장되었습니다: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b1b9a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거된 CSV 파일이 저장되었습니다: C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어_중복제거(LDA기반).csv\n"
     ]
    }
   ],
   "source": [
    "input_csv_path =r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어(LDA기반).csv\"\n",
    "\n",
    "output_csv_path =r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어_중복제거(LDA기반).csv\"\n",
    "\n",
    "# CSV 파일 읽어오기\n",
    "book_data = pd.read_csv(input_csv_path, encoding=\"cp949\")\n",
    "\n",
    "# 중복 제거 (전체 데이터프레임 기준)\n",
    "book_data_no_duplicates = book_data.drop_duplicates()\n",
    "\n",
    "# 중복 제거된 결과를 새로운 CSV 파일로 저장\n",
    "book_data_no_duplicates.to_csv(output_csv_path, index=False, encoding='cp949')\n",
    "\n",
    "print(f\"중복 제거된 CSV 파일이 저장되었습니다: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4acda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
