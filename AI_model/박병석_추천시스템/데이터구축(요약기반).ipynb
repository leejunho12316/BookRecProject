{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1541df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 입니다\n",
      "2번째 입니다\n",
      "3번째 입니다\n",
      "4번째 입니다\n",
      "5번째 입니다\n",
      "6번째 입니다\n",
      "7번째 입니다\n",
      "8번째 입니다\n",
      "9번째 입니다\n",
      "10번째 입니다\n",
      "11번째 입니다\n",
      "12번째 입니다\n",
      "13번째 입니다\n",
      "14번째 입니다\n",
      "15번째 입니다\n",
      "16번째 입니다\n",
      "17번째 입니다\n",
      "18번째 입니다\n",
      "19번째 입니다\n",
      "20번째 입니다\n",
      "21번째 입니다\n",
      "22번째 입니다\n",
      "23번째 입니다\n",
      "24번째 입니다\n",
      "25번째 입니다\n",
      "26번째 입니다\n",
      "27번째 입니다\n",
      "28번째 입니다\n",
      "29번째 입니다\n",
      "30번째 입니다\n",
      "31번째 입니다\n",
      "32번째 입니다\n",
      "33번째 입니다\n",
      "34번째 입니다\n",
      "35번째 입니다\n",
      "36번째 입니다\n",
      "37번째 입니다\n",
      "38번째 입니다\n",
      "39번째 입니다\n",
      "40번째 입니다\n",
      "41번째 입니다\n",
      "42번째 입니다\n",
      "43번째 입니다\n",
      "44번째 입니다\n",
      "45번째 입니다\n",
      "46번째 입니다\n",
      "47번째 입니다\n",
      "48번째 입니다\n",
      "49번째 입니다\n",
      "50번째 입니다\n",
      "51번째 입니다\n",
      "52번째 입니다\n",
      "53번째 입니다\n",
      "54번째 입니다\n",
      "55번째 입니다\n",
      "56번째 입니다\n",
      "57번째 입니다\n",
      "58번째 입니다\n",
      "59번째 입니다\n",
      "60번째 입니다\n",
      "61번째 입니다\n",
      "62번째 입니다\n",
      "63번째 입니다\n",
      "64번째 입니다\n",
      "65번째 입니다\n",
      "66번째 입니다\n",
      "67번째 입니다\n",
      "68번째 입니다\n",
      "69번째 입니다\n",
      "70번째 입니다\n",
      "71번째 입니다\n",
      "72번째 입니다\n",
      "73번째 입니다\n",
      "74번째 입니다\n",
      "75번째 입니다\n",
      "76번째 입니다\n",
      "77번째 입니다\n",
      "78번째 입니다\n",
      "79번째 입니다\n",
      "80번째 입니다\n",
      "81번째 입니다\n",
      "82번째 입니다\n",
      "83번째 입니다\n",
      "84번째 입니다\n",
      "85번째 입니다\n",
      "86번째 입니다\n",
      "87번째 입니다\n",
      "88번째 입니다\n",
      "89번째 입니다\n",
      "90번째 입니다\n",
      "91번째 입니다\n",
      "92번째 입니다\n",
      "93번째 입니다\n",
      "94번째 입니다\n",
      "95번째 입니다\n",
      "96번째 입니다\n",
      "97번째 입니다\n",
      "98번째 입니다\n",
      "99번째 입니다\n",
      "100번째 입니다\n",
      "101번째 입니다\n",
      "102번째 입니다\n",
      "103번째 입니다\n",
      "104번째 입니다\n",
      "105번째 입니다\n",
      "106번째 입니다\n",
      "107번째 입니다\n",
      "108번째 입니다\n",
      "109번째 입니다\n",
      "110번째 입니다\n",
      "111번째 입니다\n",
      "112번째 입니다\n",
      "113번째 입니다\n",
      "114번째 입니다\n",
      "115번째 입니다\n",
      "116번째 입니다\n",
      "117번째 입니다\n",
      "118번째 입니다\n",
      "119번째 입니다\n",
      "120번째 입니다\n",
      "121번째 입니다\n",
      "122번째 입니다\n",
      "123번째 입니다\n",
      "124번째 입니다\n",
      "125번째 입니다\n",
      "126번째 입니다\n",
      "127번째 입니다\n",
      "128번째 입니다\n",
      "129번째 입니다\n",
      "130번째 입니다\n",
      "131번째 입니다\n",
      "132번째 입니다\n",
      "133번째 입니다\n",
      "134번째 입니다\n",
      "135번째 입니다\n",
      "136번째 입니다\n",
      "137번째 입니다\n",
      "138번째 입니다\n",
      "139번째 입니다\n",
      "140번째 입니다\n",
      "141번째 입니다\n",
      "142번째 입니다\n",
      "143번째 입니다\n",
      "144번째 입니다\n",
      "145번째 입니다\n",
      "146번째 입니다\n",
      "147번째 입니다\n",
      "148번째 입니다\n",
      "149번째 입니다\n",
      "150번째 입니다\n",
      "151번째 입니다\n",
      "152번째 입니다\n",
      "153번째 입니다\n",
      "154번째 입니다\n",
      "155번째 입니다\n",
      "156번째 입니다\n",
      "157번째 입니다\n",
      "158번째 입니다\n",
      "159번째 입니다\n",
      "160번째 입니다\n",
      "161번째 입니다\n",
      "162번째 입니다\n",
      "163번째 입니다\n",
      "164번째 입니다\n",
      "165번째 입니다\n",
      "166번째 입니다\n",
      "167번째 입니다\n",
      "168번째 입니다\n",
      "169번째 입니다\n",
      "170번째 입니다\n",
      "171번째 입니다\n",
      "172번째 입니다\n",
      "173번째 입니다\n",
      "174번째 입니다\n",
      "175번째 입니다\n",
      "176번째 입니다\n",
      "177번째 입니다\n",
      "178번째 입니다\n",
      "179번째 입니다\n",
      "180번째 입니다\n",
      "181번째 입니다\n",
      "182번째 입니다\n",
      "183번째 입니다\n",
      "184번째 입니다\n",
      "185번째 입니다\n",
      "186번째 입니다\n",
      "187번째 입니다\n",
      "188번째 입니다\n",
      "189번째 입니다\n",
      "190번째 입니다\n",
      "191번째 입니다\n",
      "192번째 입니다\n",
      "193번째 입니다\n",
      "194번째 입니다\n",
      "195번째 입니다\n",
      "196번째 입니다\n",
      "197번째 입니다\n",
      "198번째 입니다\n",
      "199번째 입니다\n",
      "200번째 입니다\n",
      "201번째 입니다\n",
      "202번째 입니다\n",
      "203번째 입니다\n",
      "204번째 입니다\n",
      "205번째 입니다\n",
      "206번째 입니다\n",
      "207번째 입니다\n",
      "208번째 입니다\n",
      "209번째 입니다\n",
      "210번째 입니다\n",
      "211번째 입니다\n",
      "212번째 입니다\n",
      "213번째 입니다\n",
      "214번째 입니다\n",
      "215번째 입니다\n",
      "216번째 입니다\n",
      "217번째 입니다\n",
      "218번째 입니다\n",
      "219번째 입니다\n",
      "220번째 입니다\n",
      "221번째 입니다\n",
      "222번째 입니다\n",
      "223번째 입니다\n",
      "224번째 입니다\n",
      "225번째 입니다\n",
      "226번째 입니다\n",
      "227번째 입니다\n",
      "228번째 입니다\n",
      "229번째 입니다\n",
      "230번째 입니다\n",
      "231번째 입니다\n",
      "232번째 입니다\n",
      "233번째 입니다\n",
      "234번째 입니다\n",
      "235번째 입니다\n",
      "236번째 입니다\n",
      "237번째 입니다\n",
      "238번째 입니다\n",
      "239번째 입니다\n",
      "240번째 입니다\n",
      "241번째 입니다\n",
      "242번째 입니다\n",
      "243번째 입니다\n",
      "244번째 입니다\n",
      "245번째 입니다\n",
      "246번째 입니다\n",
      "247번째 입니다\n",
      "248번째 입니다\n",
      "249번째 입니다\n",
      "250번째 입니다\n",
      "251번째 입니다\n",
      "252번째 입니다\n",
      "253번째 입니다\n",
      "254번째 입니다\n",
      "255번째 입니다\n",
      "256번째 입니다\n",
      "257번째 입니다\n",
      "258번째 입니다\n",
      "259번째 입니다\n",
      "260번째 입니다\n",
      "저장 완료\n"
     ]
    }
   ],
   "source": [
    "#최종모델\n",
    "#설치\n",
    "#생성 요약 모델 kobart\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"EbanLee/kobart-summary-v3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EbanLee/kobart-summary-v3\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"EbanLee/kobart-summary-v3\")\n",
    "\n",
    "#데이터 불러오기\n",
    "book_data = pd.read_csv(r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\교보크롤링(소설).csv\", encoding = \"cp949\")\n",
    "\n",
    "#한국어 불용어 제거 txt 가져오기\n",
    "stop_word_path = r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\stopword.txt\"\n",
    "with open(stop_word_path, \"r\", encoding='utf-8') as file:\n",
    "    stop_words = file.read().splitlines()\n",
    "    \n",
    "#특수문자 숫자 제거\n",
    "def remove_char(text):\n",
    "    # 한국어 문자와 공백만 허용\n",
    "    # 정규식으로 찾은 문자열을 ' '공백 처리\n",
    "    text = re.sub(r'[^\\w\\s가-힣]', ' ', text)  # 특수 문자제거\n",
    "    #text = re.sub(r'\\d+', ' ', text)  # 숫자를 제거\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    filtered_words = [word for word in words if len(word)>1]\n",
    "    \n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "\n",
    "    # 원래의 공백을 복원하기 위해 공백을 원래대로 유지하는 로직 추가\n",
    "    # 단어 사이의 공백을 원래 상태로 유지\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "#불용어 제거 함수\n",
    "def remove_stop_words(list, stop_words):\n",
    "    \n",
    "    return [word for word in list if word not in stop_words]\n",
    "\n",
    "# 인코딩\n",
    "#텍스트 입력 및 토큰화\n",
    "#데이터 넣는 곳\n",
    "def summarize_extract_nouns(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1026)\n",
    "    print(f\"{i+1}번째 입니다\")\n",
    "\n",
    "    # 요약 세부 설정\n",
    "    summary_text_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    length_penalty=1.0,\n",
    "    max_length=500,\n",
    "    min_length=12,\n",
    "    num_beams=6,\n",
    "    repetition_penalty=1.5,\n",
    "    no_repeat_ngram_size=15,\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    clean_text = remove_char(summary)\n",
    "    \n",
    "    extract_nouns = okt.nouns(clean_text)\n",
    "    \n",
    "    filtered_nouns = remove_stop_words(extract_nouns , stop_words)\n",
    "    \n",
    "    final_word_set = set(noun for noun in filtered_nouns if len(noun)>1)\n",
    "    \n",
    "    return list(final_word_set)\n",
    "    \n",
    "book_data[\"추천단어\"] = \"\"\n",
    "\n",
    "for i in range(len(book_data)):\n",
    "    book_intro_data = book_data.iloc[i,4]\n",
    "    if pd.notna(book_intro_data):  # NaN 값이 아닐 때만 처리\n",
    "        nouns = summarize_extract_nouns(book_intro_data)\n",
    "        book_data.at[i, 'nouns'] = \",\".join(nouns)  # 리스트를 문자열로 변환하여 저장\n",
    "\n",
    "# 결과 저장\n",
    "output_path = r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\교보크롤링_소설요약.csv\"\n",
    "book_data.to_csv(output_path, index=False, encoding=\"cp949\")\n",
    "print(\"저장 완료\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affe0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거된 CSV 파일이 저장되었습니다: C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어_중복제거(요약기반).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_csv_path =r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어(요약기반).csv\"\n",
    "\n",
    "output_csv_path =r\"C:\\Users\\user\\Desktop\\졸업 프로젝트\\인공지능(데이터,모델)\\추천단어_중복제거(요약기반).csv\"\n",
    "\n",
    "# CSV 파일 읽어오기\n",
    "book_data = pd.read_csv(input_csv_path, encoding=\"cp949\")\n",
    "\n",
    "# 중복 제거 (전체 데이터프레임 기준)\n",
    "book_data_no_duplicates = book_data.drop_duplicates()\n",
    "\n",
    "# 중복 제거된 결과를 새로운 CSV 파일로 저장\n",
    "book_data_no_duplicates.to_csv(output_csv_path, index=False, encoding='cp949')\n",
    "\n",
    "print(f\"중복 제거된 CSV 파일이 저장되었습니다: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
